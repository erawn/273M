{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Computational Social Science] Project 5: Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, you will use natural language processing techniques to explore a dataset containing tweets from members of the 116th United States Congress that met from January 3, 2019 to January 2, 2021. The dataset has also been cleaned to contain information about each legislator. Concretely, you will do the following:\n",
    "\n",
    "* Preprocess the text of legislators' tweets\n",
    "* Conduct Exploratory Data Analysis of the text\n",
    "* Use sentiment analysis to explore differences between legislators' tweets\n",
    "* Featurize text with manual feature engineering, frequency-based, and vector-based techniques\n",
    "* Predict legislators' political parties and whether they are a Senator or Representative\n",
    "\n",
    "You will explore two questions that relate to two central findings in political science and examine how they relate to the text of legislators' tweets. First, political scientists have argued that U.S. politics is currently highly polarized relative to other periods in American history, but also that the polarization is asymmetric. Historically, there were several conservative Democrats (i.e. \"blue dog Democrats\") and liberal Republicans (i.e. \"Rockefeller Republicans\"), as measured by popular measurement tools like [DW-NOMINATE](https://en.wikipedia.org/wiki/NOMINATE_(scaling_method)#:~:text=DW\\%2DNOMINATE\\%20scores\\%20have\\%20been,in\\%20the\\%20liberal\\%2Dconservative\\%20scale.). However, in the last few years, there are few if any examples of any Democrat in Congress being further to the right than any Republican and vice versa. At the same time, scholars have argued that this polarization is mostly a function of the Republican party moving further right than the Democratic party has moved left. **Does this sort of asymmetric polarization show up in how politicians communicate to their constituents through tweets?**\n",
    "\n",
    "Second, the U.S. Congress is a bicameral legislature, and there has long been debate about partisanship in the Senate versus the House. The House of Representatives is apportioned by population and all members serve two year terms. In the Senate, each state receives two Senators and each Senator serves a term of six years. For a variety of reasons (smaller chamber size, more insulation from the voters, rules and norms like the filibuster, etc.), the Senate has been argued to be the \"cooling saucer\" of Congress in that it is more bipartisan and moderate than the House. **Does the theory that the Senate is more moderate have support in Senators' tweets?**\n",
    "\n",
    "**Note**: See the project handout for more details on caveats and the data dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas and numpy\n",
    "import pandas as pd\n",
    "import numpy as numpy\n",
    "\n",
    "# punctuation, stop words and English language model\n",
    "from string import punctuation\n",
    "from spacy.tokens import DocBin\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "# textblob\n",
    "from textblob import TextBlob\n",
    "\n",
    "# countvectorizer, tfidfvectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "# gensim\n",
    "import gensim\n",
    "from gensim import models\n",
    "\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "\n",
    "import scattertext as st\n",
    "import pickle\n",
    "import polars as pl\n",
    "import pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data \n",
    "# ----------\n",
    "congress_tweets = pd.read_csv(\"../../data/116th Congressional Tweets and Demographics.csv\")\n",
    "# fill in this line of code with a sufficient number of tweets, depending on your computational resources\n",
    "#congress_tweets = congress_tweets.sample(...)\n",
    "congress_tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step in working with text data is to preprocess it. Make sure you do the following:\n",
    "\n",
    "* Remove punctuation and stop words. The `rem_punc_stop()` function we used in lab is provided to you but you should feel free to edit it as necessary for other steps\n",
    "* Remove tokens that occur frequently in tweets, but may not be helpful for downstream classification. For instance, many tweets contain a flag for retweeting, or share a URL \n",
    "\n",
    "As you search online, you might run into solutions that rely on regular expressions. You are free to use these, but you should also be able to preprocess using the techniques we covered in lab. Specifically, we encourage you to use spaCy's token attributes and string methods to do some of this text preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_model(text):\n",
    "    \n",
    "    punc = set(punctuation)\n",
    "    \n",
    "    punc_free = \"\".join([ch for ch in text if ch not in punc])\n",
    "    return punc_free\n",
    "\n",
    "def post_model(doc):\n",
    "    stop_words = STOP_WORDS\n",
    "    spacy_words = [token.text for token in doc if not token.like_url]\n",
    "    \n",
    "    spacy_words = [word for word in spacy_words if not (word.startswith('http') or word.startswith(\"RT\") or word.startswith(\"QT\"))]\n",
    "    \n",
    "    no_punc = [word for word in spacy_words if word not in stop_words]\n",
    "    \n",
    "    return no_punc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pre_model_data = congress_tweets['text'].map(pre_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#docs = list(nlp.pipe(pre_model_data,n_process=4, batch_size=4000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc_bin = DocBin(docs=docs)\n",
    "# doc_bin.to_disk(\"./data.spacy\")\n",
    "doc_bin = DocBin().from_disk(\"./data.spacy\")\n",
    "new_docs = list(doc_bin.get_docs(nlp.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# post_model_data = [post_model(doc) for doc in new_docs]\n",
    "# congress_tweets['tokens'] = post_model_data\n",
    "# congress_tweets['joined_tokens'] = congress_tweets['tokens'].map(lambda text: ' '.join(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# congress_tweets.head()\n",
    "# congress_tweets.to_pickle('cleanedData.pkl')\n",
    "congress_tweets = pd.read_pickle(\"cleanedData.pkl\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "congress_tweets['party'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use two of the techniques we covered in lab (or other techniques outside of lab!) to explore the text of the tweets. You should construct these visualizations with an eye toward the eventual classification tasks: (1) predicting the legislator's political party based on the text of their tweet, and (2) predicting whether the legislator is a Senator or Representative. As a reminder, in lab we covered word frequencies, word clouds, word/character counts, scattertext, and topic modeling as possible exploration tools. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud_text = ' '.join(congress_tweets['joined_tokens'])\n",
    "wordcloud = WordCloud(background_color=\"white\",random_state=111).generate(wordcloud_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scatter_tweets = congress_tweets\n",
    "scatter_tweets['doc'] = new_docs\n",
    "corpus = st.CorpusFromParsedDocuments(congress_tweets, category_col=\"party\", parsed_col=\"doc\").build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = st.produce_scattertext_explorer(corpus,\n",
    "                                       category='Democrat',\n",
    "                                       category_name='Democratic',\n",
    "                                       not_category_name='Republican',\n",
    "                                       width_in_pixels=1000,\n",
    "                                       minimum_term_frequency=5,\n",
    "                                       metadata=congress_tweets['screen_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "open(\"PoliticalTweets.html\", 'wb').write(html.encode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's analyze the sentiments contained within the tweets. You may use TextBlob or another library for these tasks. Do the following:\n",
    "\n",
    "* Choose two legislators, one who you think will be more liberal and one who you think will be more conservative, and analyze their sentiment and/or subjectivity scores per tweet. For instance, you might do two scatterplots that plot each legislator's sentiment against their subjectivity, or two density plots for their sentiments. Do the scores match what you thought?\n",
    "* Plot two more visualizations like the ones you chose in the first part, but do them to compare (1) Democrats v. Republicans and (2) Senators v. Representatives \n",
    "\n",
    "`TextBlob` has already been imported in the top cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 20)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>tweet_id</th><th>screen_name</th><th>datetime</th><th>text</th><th>name_wikipedia</th><th>position</th><th>joined_congress_date</th><th>birthday</th><th>gender</th><th>state</th><th>district_number</th><th>party</th><th>trump_2016_state_share</th><th>clinton_2016_state_share</th><th>obama_2012_state_share</th><th>romney_2012_state_share</th><th>tokens</th><th>joined_tokens</th><th>polarity</th><th>subjectivity</th></tr><tr><td>f64</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>list[str]</td><td>str</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>1.0810e18</td><td>&quot;RepByrne&quot;</td><td>&quot;2019-01-03T21:23:00-05:00&quot;</td><td>&quot;Great news for Baldwin County!…</td><td>&quot;Bradley Byrne&quot;</td><td>&quot;Rep&quot;</td><td>&quot;8-Jan-14&quot;</td><td>&quot;2/16/1955&quot;</td><td>&quot;M&quot;</td><td>&quot;AL&quot;</td><td>&quot;1&quot;</td><td>&quot;Republican&quot;</td><td>&quot;1,318,255&quot;</td><td>&quot;729,547&quot;</td><td>&quot;795,696&quot;</td><td>&quot;1,255,925&quot;</td><td>[&quot;Great&quot;, &quot;news&quot;, … &quot;\n",
       "&quot;]</td><td>&quot;Great news Baldwin County The …</td><td>0.6</td><td>0.65</td></tr><tr><td>1.0809e18</td><td>&quot;RepByrne&quot;</td><td>&quot;2019-01-03T12:30:38-05:00&quot;</td><td>&quot;Outstanding news today from @A…</td><td>&quot;Bradley Byrne&quot;</td><td>&quot;Rep&quot;</td><td>&quot;8-Jan-14&quot;</td><td>&quot;2/16/1955&quot;</td><td>&quot;M&quot;</td><td>&quot;AL&quot;</td><td>&quot;1&quot;</td><td>&quot;Republican&quot;</td><td>&quot;1,318,255&quot;</td><td>&quot;729,547&quot;</td><td>&quot;795,696&quot;</td><td>&quot;1,255,925&quot;</td><td>[&quot;Outstanding&quot;, &quot;news&quot;, … &quot;\n",
       "&quot;]</td><td>&quot;Outstanding news today Airbus …</td><td>0.5</td><td>0.75</td></tr><tr><td>1.0808e18</td><td>&quot;RepByrne&quot;</td><td>&quot;2019-01-03T09:12:07-05:00&quot;</td><td>&quot;RT @senatemajldr Democrats wil…</td><td>&quot;Bradley Byrne&quot;</td><td>&quot;Rep&quot;</td><td>&quot;8-Jan-14&quot;</td><td>&quot;2/16/1955&quot;</td><td>&quot;M&quot;</td><td>&quot;AL&quot;</td><td>&quot;1&quot;</td><td>&quot;Republican&quot;</td><td>&quot;1,318,255&quot;</td><td>&quot;729,547&quot;</td><td>&quot;795,696&quot;</td><td>&quot;1,255,925&quot;</td><td>[&quot;senatemajldr&quot;, &quot;Democrats&quot;, … &quot;solve&quot;]</td><td>&quot;senatemajldr Democrats border …</td><td>0.0</td><td>0.0</td></tr><tr><td>1.0809e18</td><td>&quot;RepByrne&quot;</td><td>&quot;2019-01-03T13:20:53-05:00&quot;</td><td>&quot;Here is a sign of things to co…</td><td>&quot;Bradley Byrne&quot;</td><td>&quot;Rep&quot;</td><td>&quot;8-Jan-14&quot;</td><td>&quot;2/16/1955&quot;</td><td>&quot;M&quot;</td><td>&quot;AL&quot;</td><td>&quot;1&quot;</td><td>&quot;Republican&quot;</td><td>&quot;1,318,255&quot;</td><td>&quot;729,547&quot;</td><td>&quot;795,696&quot;</td><td>&quot;1,255,925&quot;</td><td>[&quot;Here&quot;, &quot;sign&quot;, … &quot;Congress&quot;]</td><td>&quot;Here sign things come As Democ…</td><td>-0.024242</td><td>0.413636</td></tr><tr><td>1.0809e18</td><td>&quot;RepByrne&quot;</td><td>&quot;2019-01-03T12:10:26-05:00&quot;</td><td>&quot;Let&#x27;s understand what we&#x27;re de…</td><td>&quot;Bradley Byrne&quot;</td><td>&quot;Rep&quot;</td><td>&quot;8-Jan-14&quot;</td><td>&quot;2/16/1955&quot;</td><td>&quot;M&quot;</td><td>&quot;AL&quot;</td><td>&quot;1&quot;</td><td>&quot;Republican&quot;</td><td>&quot;1,318,255&quot;</td><td>&quot;729,547&quot;</td><td>&quot;795,696&quot;</td><td>&quot;1,255,925&quot;</td><td>[&quot;Lets&quot;, &quot;understand&quot;, … &quot;security&quot;]</td><td>&quot;Lets understand dealing border…</td><td>0.0</td><td>0.0</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 20)\n",
       "┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬──────────┐\n",
       "│ tweet_id  ┆ screen_na ┆ datetime  ┆ text      ┆ … ┆ tokens    ┆ joined_to ┆ polarity  ┆ subjecti │\n",
       "│ ---       ┆ me        ┆ ---       ┆ ---       ┆   ┆ ---       ┆ kens      ┆ ---       ┆ vity     │\n",
       "│ f64       ┆ ---       ┆ str       ┆ str       ┆   ┆ list[str] ┆ ---       ┆ f64       ┆ ---      │\n",
       "│           ┆ str       ┆           ┆           ┆   ┆           ┆ str       ┆           ┆ f64      │\n",
       "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪══════════╡\n",
       "│ 1.0810e18 ┆ RepByrne  ┆ 2019-01-0 ┆ Great     ┆ … ┆ [\"Great\", ┆ Great     ┆ 0.6       ┆ 0.65     │\n",
       "│           ┆           ┆ 3T21:23:0 ┆ news for  ┆   ┆ \"news\", … ┆ news      ┆           ┆          │\n",
       "│           ┆           ┆ 0-05:00   ┆ Baldwin   ┆   ┆ \"         ┆ Baldwin   ┆           ┆          │\n",
       "│           ┆           ┆           ┆ County!…  ┆   ┆ \"]        ┆ County    ┆           ┆          │\n",
       "│           ┆           ┆           ┆           ┆   ┆           ┆ The …     ┆           ┆          │\n",
       "│ 1.0809e18 ┆ RepByrne  ┆ 2019-01-0 ┆ Outstandi ┆ … ┆ [\"Outstan ┆ Outstandi ┆ 0.5       ┆ 0.75     │\n",
       "│           ┆           ┆ 3T12:30:3 ┆ ng news   ┆   ┆ ding\",    ┆ ng news   ┆           ┆          │\n",
       "│           ┆           ┆ 8-05:00   ┆ today     ┆   ┆ \"news\", … ┆ today     ┆           ┆          │\n",
       "│           ┆           ┆           ┆ from @A…  ┆   ┆ \"         ┆ Airbus …  ┆           ┆          │\n",
       "│           ┆           ┆           ┆           ┆   ┆ \"]        ┆           ┆           ┆          │\n",
       "│ 1.0808e18 ┆ RepByrne  ┆ 2019-01-0 ┆ RT @senat ┆ … ┆ [\"senatem ┆ senatemaj ┆ 0.0       ┆ 0.0      │\n",
       "│           ┆           ┆ 3T09:12:0 ┆ emajldr   ┆   ┆ ajldr\",   ┆ ldr       ┆           ┆          │\n",
       "│           ┆           ┆ 7-05:00   ┆ Democrats ┆   ┆ \"Democrat ┆ Democrats ┆           ┆          │\n",
       "│           ┆           ┆           ┆ wil…      ┆   ┆ s\", …     ┆ border …  ┆           ┆          │\n",
       "│ 1.0809e18 ┆ RepByrne  ┆ 2019-01-0 ┆ Here is a ┆ … ┆ [\"Here\",  ┆ Here sign ┆ -0.024242 ┆ 0.413636 │\n",
       "│           ┆           ┆ 3T13:20:5 ┆ sign of   ┆   ┆ \"sign\", … ┆ things    ┆           ┆          │\n",
       "│           ┆           ┆ 3-05:00   ┆ things to ┆   ┆ \"Congress ┆ come As   ┆           ┆          │\n",
       "│           ┆           ┆           ┆ co…       ┆   ┆ \"]        ┆ Democ…    ┆           ┆          │\n",
       "│ 1.0809e18 ┆ RepByrne  ┆ 2019-01-0 ┆ Let's und ┆ … ┆ [\"Lets\",  ┆ Lets unde ┆ 0.0       ┆ 0.0      │\n",
       "│           ┆           ┆ 3T12:10:2 ┆ erstand   ┆   ┆ \"understa ┆ rstand    ┆           ┆          │\n",
       "│           ┆           ┆ 6-05:00   ┆ what      ┆   ┆ nd\", …    ┆ dealing   ┆           ┆          │\n",
       "│           ┆           ┆           ┆ we're de… ┆   ┆ \"secu…    ┆ border…   ┆           ┆          │\n",
       "└───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴──────────┘"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# congress_tweets['polarity'] = congress_tweets['joined_tokens'].map(lambda x: TextBlob(x).sentiment.polarity)\n",
    "# congress_tweets['subjectivity'] = congress_tweets['joined_tokens'].map(lambda x: TextBlob(x).sentiment.subjectivity)\n",
    "#congress_tweets.drop(columns=['doc']).to_pickle('sentimentData.pkl')\n",
    "congress_tweets = pd.read_pickle(\"sentimentData.pkl\") \n",
    "tweets = pl.from_pandas(congress_tweets)\n",
    "tweets = tweets.filter((pl.col('party') == \"Republican\") | (pl.col(\"party\") == \"Democrat\"))\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_sen = tweets.filter(pl.col(\"name_wikipedia\").is_in([\"Ted Cruz\", \"Maxine Waters\"])).select(\"name_wikipedia\",\"subjectivity\",\"polarity\",\"tweet_id\")\n",
    "sns.scatterplot(\n",
    "   two_sen,\n",
    "    x=\"subjectivity\",\n",
    "    y=\"polarity\",\n",
    "    hue=\"name_wikipedia\",\n",
    "    alpha=.3\n",
    ")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(two_sen,\n",
    "            x=\"polarity\",\n",
    "            hue=\"name_wikipedia\",\n",
    "            )\n",
    "plt.show()\n",
    "sns.kdeplot(two_sen,\n",
    "            x=\"subjectivity\",\n",
    "            hue=\"name_wikipedia\"\n",
    "            )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(tweets.select([\"tweet_id\",\"party\",\"polarity\",\"subjectivity\"]),\n",
    "            x=\"polarity\",\n",
    "            y=\"subjectivity\",\n",
    "            hue=\"party\",\n",
    "            alpha=.1\n",
    "            )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(tweets.select([\"tweet_id\",\"party\",\"polarity\",\"subjectivity\"]),\n",
    "            x=\"polarity\",\n",
    "            hue=\"party\",\n",
    "            )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Featurization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before going to classification, explore different featurization techniques. Create three dataframes or arrays to represent your text features, specifically:\n",
    "\n",
    "* Features engineered from your previous analysis. For example, word counts, sentiment scores, topic model etc.\n",
    "* A term frequency-inverse document frequency matrix. \n",
    "* An embedding-based featurization (like a document averaged word2vec)\n",
    "\n",
    "In the next section, you will experiment with each of these featurization techniques to see which one produces the best classifications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag-of-words or Tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = TfidfVectorizer()\n",
    "token_list = pl.Series(tweets.select('joined_tokens')).to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequency Based featurization\n",
    "tfidf_matrix = tf.fit_transform(token_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_words(model, feature_names, n_top_words):   # define function and parameters\n",
    "    for topic_idx, topic in enumerate(model.components_): # iterate over each topic \n",
    "        print(\"\\nTopic #{}:\".format(topic_idx))           # print topic index\n",
    "        print(\" \".join([feature_names[i]                  # print topics\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic #0:\n",
      "amp the today we this house act health care covid19 americans people need work it trump american help bill new\n",
      "\n",
      "Topic #1:\n",
      "amp we the today health help trump people president work covid19 senate this americans act need care house us time\n",
      "\n",
      "Topic #2:\n",
      "amp we the time today need president trump act this work american covid19 workers help health thank families new house\n",
      "\n",
      "Topic #3:\n",
      "amp the today this president we trump it house need congress people thank day bill support health work american america\n",
      "\n",
      "Topic #4:\n",
      "amp the today people we this act covid19 trump coronavirus american new need house president congress health great national bill\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lda = LatentDirichletAllocation(n_components=5,\n",
    "                                max_iter=1,\n",
    "                                random_state=0).fit(tfidf_matrix)\n",
    "tf_feature_names = tf.get_feature_names_out()\n",
    "\n",
    "# apply function to print top 20 words\n",
    "print_top_words(lda,                # model \n",
    "                tf_feature_names,   # feature names \n",
    "                20)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_dist = lda.transform(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Engineered Text Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engineered_features = tweets.with_columns(\n",
    "    word_count= (pl.col('tokens').list.len()),\n",
    "    length=(pl.col('joined_tokens').str.len_chars())\n",
    "    ).select([\"word_count\",\"length\",\"polarity\",\"subjectivity\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/GoogleNews-vectors-negative300.bin.gz'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Load Word2Vec model from Google; OPTIONAL depending on your computational resources (the file is ~1 GB)\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Also note that this file path assumes that the word vectors are underneath 'data'; you may wish to point to the CSS course repo and change the path\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# or move the vector file to the project repo \u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mgensim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mKeyedVectors\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_word2vec_format\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata/GoogleNews-vectors-negative300.bin.gz\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbinary\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m \n",
      "File \u001b[0;32m~/Computational-Social-Science-Training-Program/env/lib/python3.10/site-packages/gensim/models/keyedvectors.py:1719\u001b[0m, in \u001b[0;36mKeyedVectors.load_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header)\u001b[0m\n\u001b[1;32m   1672\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m   1673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_word2vec_format\u001b[39m(\n\u001b[1;32m   1674\u001b[0m         \u001b[38;5;28mcls\u001b[39m, fname, fvocab\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf8\u001b[39m\u001b[38;5;124m'\u001b[39m, unicode_errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   1675\u001b[0m         limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, datatype\u001b[38;5;241m=\u001b[39mREAL, no_header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1676\u001b[0m     ):\n\u001b[1;32m   1677\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load KeyedVectors from a file produced by the original C word2vec-tool format.\u001b[39;00m\n\u001b[1;32m   1678\u001b[0m \n\u001b[1;32m   1679\u001b[0m \u001b[38;5;124;03m    Warnings\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1717\u001b[0m \n\u001b[1;32m   1718\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1719\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load_word2vec_format\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1720\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfvocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfvocab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbinary\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbinary\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43municode_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43municode_errors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1721\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatatype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdatatype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mno_header\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mno_header\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1722\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Computational-Social-Science-Training-Program/env/lib/python3.10/site-packages/gensim/models/keyedvectors.py:2048\u001b[0m, in \u001b[0;36m_load_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header, binary_chunk_size)\u001b[0m\n\u001b[1;32m   2045\u001b[0m             counts[word] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(count)\n\u001b[1;32m   2047\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloading projection weights from \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, fname)\n\u001b[0;32m-> 2048\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m fin:\n\u001b[1;32m   2049\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m no_header:\n\u001b[1;32m   2050\u001b[0m         \u001b[38;5;66;03m# deduce both vocab_size & vector_size from 1st pass over file\u001b[39;00m\n\u001b[1;32m   2051\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m binary:\n",
      "File \u001b[0;32m~/Computational-Social-Science-Training-Program/env/lib/python3.10/site-packages/smart_open/smart_open_lib.py:224\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(uri, mode, buffering, encoding, errors, newline, closefd, opener, compression, transport_params)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ve:\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(ve\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m--> 224\u001b[0m binary \u001b[38;5;241m=\u001b[39m \u001b[43m_open_binary_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[43muri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbinary_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransport_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    225\u001b[0m filename \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    226\u001b[0m     binary\u001b[38;5;241m.\u001b[39mname\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;66;03m# if name attribute is not string-like (e.g. ftp socket fileno)...\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    230\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m uri\n\u001b[1;32m    231\u001b[0m )\n\u001b[1;32m    232\u001b[0m decompressed \u001b[38;5;241m=\u001b[39m so_compression\u001b[38;5;241m.\u001b[39mcompression_wrapper(\n\u001b[1;32m    233\u001b[0m     binary,\n\u001b[1;32m    234\u001b[0m     binary_mode,\n\u001b[1;32m    235\u001b[0m     compression,\n\u001b[1;32m    236\u001b[0m     filename\u001b[38;5;241m=\u001b[39mfilename,\n\u001b[1;32m    237\u001b[0m )\n",
      "File \u001b[0;32m~/Computational-Social-Science-Training-Program/env/lib/python3.10/site-packages/smart_open/smart_open_lib.py:412\u001b[0m, in \u001b[0;36m_open_binary_stream\u001b[0;34m(uri, mode, transport_params)\u001b[0m\n\u001b[1;32m    410\u001b[0m scheme \u001b[38;5;241m=\u001b[39m _sniff_scheme(uri)\n\u001b[1;32m    411\u001b[0m submodule \u001b[38;5;241m=\u001b[39m transport\u001b[38;5;241m.\u001b[39mget_transport(scheme)\n\u001b[0;32m--> 412\u001b[0m fobj \u001b[38;5;241m=\u001b[39m \u001b[43msubmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen_uri\u001b[49m\u001b[43m(\u001b[49m\u001b[43muri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransport_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(fobj, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    414\u001b[0m     fobj\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m=\u001b[39m uri\n",
      "File \u001b[0;32m~/Computational-Social-Science-Training-Program/env/lib/python3.10/site-packages/smart_open/local_file.py:34\u001b[0m, in \u001b[0;36mopen_uri\u001b[0;34m(uri_as_string, mode, transport_params)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mopen_uri\u001b[39m(uri_as_string, mode, transport_params):\n\u001b[1;32m     33\u001b[0m     parsed_uri \u001b[38;5;241m=\u001b[39m parse_uri(uri_as_string)\n\u001b[0;32m---> 34\u001b[0m     fobj \u001b[38;5;241m=\u001b[39m \u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparsed_uri\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43muri_path\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fobj\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/GoogleNews-vectors-negative300.bin.gz'"
     ]
    }
   ],
   "source": [
    "# Load Word2Vec model from Google; OPTIONAL depending on your computational resources (the file is ~1 GB)\n",
    "# Also note that this file path assumes that the word vectors are underneath 'data'; you may wish to point to the CSS course repo and change the path\n",
    "# or move the vector file to the project repo \n",
    "\n",
    "#model = gensim.models.KeyedVectors.load_word2vec_format('data/GoogleNews-vectors-negative300.bin.gz', binary = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to average word embeddings for a document; use examples from lab to apply this function. You can use also other techniques such as PCA and doc2vec instead.\n",
    "def document_vector(word2vec_model, doc):\n",
    "    doc = [word for word in doc if word in model.vocab]\n",
    "    return np.mean(model[doc], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding based featurization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 20)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>tweet_id</th><th>screen_name</th><th>datetime</th><th>text</th><th>name_wikipedia</th><th>position</th><th>joined_congress_date</th><th>birthday</th><th>gender</th><th>state</th><th>district_number</th><th>party</th><th>trump_2016_state_share</th><th>clinton_2016_state_share</th><th>obama_2012_state_share</th><th>romney_2012_state_share</th><th>tokens</th><th>joined_tokens</th><th>polarity</th><th>subjectivity</th></tr><tr><td>f64</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>list[str]</td><td>str</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>1.0810e18</td><td>&quot;RepByrne&quot;</td><td>&quot;2019-01-03T21:23:00-05:00&quot;</td><td>&quot;Great news for Baldwin County!…</td><td>&quot;Bradley Byrne&quot;</td><td>&quot;Rep&quot;</td><td>&quot;8-Jan-14&quot;</td><td>&quot;2/16/1955&quot;</td><td>&quot;M&quot;</td><td>&quot;AL&quot;</td><td>&quot;1&quot;</td><td>&quot;Republican&quot;</td><td>&quot;1,318,255&quot;</td><td>&quot;729,547&quot;</td><td>&quot;795,696&quot;</td><td>&quot;1,255,925&quot;</td><td>[&quot;Great&quot;, &quot;news&quot;, … &quot;\n",
       "&quot;]</td><td>&quot;Great news Baldwin County The …</td><td>0.6</td><td>0.65</td></tr><tr><td>1.0809e18</td><td>&quot;RepByrne&quot;</td><td>&quot;2019-01-03T12:30:38-05:00&quot;</td><td>&quot;Outstanding news today from @A…</td><td>&quot;Bradley Byrne&quot;</td><td>&quot;Rep&quot;</td><td>&quot;8-Jan-14&quot;</td><td>&quot;2/16/1955&quot;</td><td>&quot;M&quot;</td><td>&quot;AL&quot;</td><td>&quot;1&quot;</td><td>&quot;Republican&quot;</td><td>&quot;1,318,255&quot;</td><td>&quot;729,547&quot;</td><td>&quot;795,696&quot;</td><td>&quot;1,255,925&quot;</td><td>[&quot;Outstanding&quot;, &quot;news&quot;, … &quot;\n",
       "&quot;]</td><td>&quot;Outstanding news today Airbus …</td><td>0.5</td><td>0.75</td></tr><tr><td>1.0808e18</td><td>&quot;RepByrne&quot;</td><td>&quot;2019-01-03T09:12:07-05:00&quot;</td><td>&quot;RT @senatemajldr Democrats wil…</td><td>&quot;Bradley Byrne&quot;</td><td>&quot;Rep&quot;</td><td>&quot;8-Jan-14&quot;</td><td>&quot;2/16/1955&quot;</td><td>&quot;M&quot;</td><td>&quot;AL&quot;</td><td>&quot;1&quot;</td><td>&quot;Republican&quot;</td><td>&quot;1,318,255&quot;</td><td>&quot;729,547&quot;</td><td>&quot;795,696&quot;</td><td>&quot;1,255,925&quot;</td><td>[&quot;senatemajldr&quot;, &quot;Democrats&quot;, … &quot;solve&quot;]</td><td>&quot;senatemajldr Democrats border …</td><td>0.0</td><td>0.0</td></tr><tr><td>1.0809e18</td><td>&quot;RepByrne&quot;</td><td>&quot;2019-01-03T13:20:53-05:00&quot;</td><td>&quot;Here is a sign of things to co…</td><td>&quot;Bradley Byrne&quot;</td><td>&quot;Rep&quot;</td><td>&quot;8-Jan-14&quot;</td><td>&quot;2/16/1955&quot;</td><td>&quot;M&quot;</td><td>&quot;AL&quot;</td><td>&quot;1&quot;</td><td>&quot;Republican&quot;</td><td>&quot;1,318,255&quot;</td><td>&quot;729,547&quot;</td><td>&quot;795,696&quot;</td><td>&quot;1,255,925&quot;</td><td>[&quot;Here&quot;, &quot;sign&quot;, … &quot;Congress&quot;]</td><td>&quot;Here sign things come As Democ…</td><td>-0.024242</td><td>0.413636</td></tr><tr><td>1.0809e18</td><td>&quot;RepByrne&quot;</td><td>&quot;2019-01-03T12:10:26-05:00&quot;</td><td>&quot;Let&#x27;s understand what we&#x27;re de…</td><td>&quot;Bradley Byrne&quot;</td><td>&quot;Rep&quot;</td><td>&quot;8-Jan-14&quot;</td><td>&quot;2/16/1955&quot;</td><td>&quot;M&quot;</td><td>&quot;AL&quot;</td><td>&quot;1&quot;</td><td>&quot;Republican&quot;</td><td>&quot;1,318,255&quot;</td><td>&quot;729,547&quot;</td><td>&quot;795,696&quot;</td><td>&quot;1,255,925&quot;</td><td>[&quot;Lets&quot;, &quot;understand&quot;, … &quot;security&quot;]</td><td>&quot;Lets understand dealing border…</td><td>0.0</td><td>0.0</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 20)\n",
       "┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬──────────┐\n",
       "│ tweet_id  ┆ screen_na ┆ datetime  ┆ text      ┆ … ┆ tokens    ┆ joined_to ┆ polarity  ┆ subjecti │\n",
       "│ ---       ┆ me        ┆ ---       ┆ ---       ┆   ┆ ---       ┆ kens      ┆ ---       ┆ vity     │\n",
       "│ f64       ┆ ---       ┆ str       ┆ str       ┆   ┆ list[str] ┆ ---       ┆ f64       ┆ ---      │\n",
       "│           ┆ str       ┆           ┆           ┆   ┆           ┆ str       ┆           ┆ f64      │\n",
       "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪══════════╡\n",
       "│ 1.0810e18 ┆ RepByrne  ┆ 2019-01-0 ┆ Great     ┆ … ┆ [\"Great\", ┆ Great     ┆ 0.6       ┆ 0.65     │\n",
       "│           ┆           ┆ 3T21:23:0 ┆ news for  ┆   ┆ \"news\", … ┆ news      ┆           ┆          │\n",
       "│           ┆           ┆ 0-05:00   ┆ Baldwin   ┆   ┆ \"         ┆ Baldwin   ┆           ┆          │\n",
       "│           ┆           ┆           ┆ County!…  ┆   ┆ \"]        ┆ County    ┆           ┆          │\n",
       "│           ┆           ┆           ┆           ┆   ┆           ┆ The …     ┆           ┆          │\n",
       "│ 1.0809e18 ┆ RepByrne  ┆ 2019-01-0 ┆ Outstandi ┆ … ┆ [\"Outstan ┆ Outstandi ┆ 0.5       ┆ 0.75     │\n",
       "│           ┆           ┆ 3T12:30:3 ┆ ng news   ┆   ┆ ding\",    ┆ ng news   ┆           ┆          │\n",
       "│           ┆           ┆ 8-05:00   ┆ today     ┆   ┆ \"news\", … ┆ today     ┆           ┆          │\n",
       "│           ┆           ┆           ┆ from @A…  ┆   ┆ \"         ┆ Airbus …  ┆           ┆          │\n",
       "│           ┆           ┆           ┆           ┆   ┆ \"]        ┆           ┆           ┆          │\n",
       "│ 1.0808e18 ┆ RepByrne  ┆ 2019-01-0 ┆ RT @senat ┆ … ┆ [\"senatem ┆ senatemaj ┆ 0.0       ┆ 0.0      │\n",
       "│           ┆           ┆ 3T09:12:0 ┆ emajldr   ┆   ┆ ajldr\",   ┆ ldr       ┆           ┆          │\n",
       "│           ┆           ┆ 7-05:00   ┆ Democrats ┆   ┆ \"Democrat ┆ Democrats ┆           ┆          │\n",
       "│           ┆           ┆           ┆ wil…      ┆   ┆ s\", …     ┆ border …  ┆           ┆          │\n",
       "│ 1.0809e18 ┆ RepByrne  ┆ 2019-01-0 ┆ Here is a ┆ … ┆ [\"Here\",  ┆ Here sign ┆ -0.024242 ┆ 0.413636 │\n",
       "│           ┆           ┆ 3T13:20:5 ┆ sign of   ┆   ┆ \"sign\", … ┆ things    ┆           ┆          │\n",
       "│           ┆           ┆ 3-05:00   ┆ things to ┆   ┆ \"Congress ┆ come As   ┆           ┆          │\n",
       "│           ┆           ┆           ┆ co…       ┆   ┆ \"]        ┆ Democ…    ┆           ┆          │\n",
       "│ 1.0809e18 ┆ RepByrne  ┆ 2019-01-0 ┆ Let's und ┆ … ┆ [\"Lets\",  ┆ Lets unde ┆ 0.0       ┆ 0.0      │\n",
       "│           ┆           ┆ 3T12:10:2 ┆ erstand   ┆   ┆ \"understa ┆ rstand    ┆           ┆          │\n",
       "│           ┆           ┆ 6-05:00   ┆ what      ┆   ┆ nd\", …    ┆ dealing   ┆           ┆          │\n",
       "│           ┆           ┆           ┆ we're de… ┆   ┆ \"secu…    ┆ border…   ┆           ┆          │\n",
       "└───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴──────────┘"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Either use cross-validation or partition your data with training/validation/test sets for this section. Do the following:\n",
    "\n",
    "* Choose a supervised learning algorithm such as logistic regression, random forest etc. \n",
    "* Train six models. For each of the three dataframes you created in the featurization part, train one model to predict whether the author of the tweet is a Democrat or Republican, and a second model to predict whether the author is a Senator or Representative.\n",
    "* Report the accuracy and other relevant metrics for each of these six models.\n",
    "* Choose the featurization technique associated with your best model. Combine those text features with non-text features. Train two more models: (1) A supervised learning algorithm that uses just the non-text features and (2) a supervised learning algorithm that combines text and non-text features. Report accuracy and other relevant metrics. \n",
    "\n",
    "If time permits, you are encouraged to use hyperparameter tuning or AutoML techniques like TPOT, but are not explicitly required to do so."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Six Models with Just Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# six models ([engineered features, frequency-based, embedding] * [democrat/republican, senator/representative])\n",
    "dataframes = [engineered_features]\n",
    "techniques = [\"Engineered Features\"]\n",
    "\n",
    "lb_style = LabelBinarizer()\n",
    "\n",
    "targets = [lb_style.fit_transform(tweets.select(pl.col('party'))), \n",
    "           lb_style.fit_transform(tweets.select(pl.col('position')))]\n",
    "\n",
    "target_labels = [\"Republican/Democrat\",\"Senator/Representative\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataframe, technique in zip(dataframes, techniques):\n",
    "    for target, target_label in zip(targets,target_labels):\n",
    "            X_train, X_test, y_train, y_test = train_test_split(dataframe,   \n",
    "                                                        target,\n",
    "                                                        train_size = .80,\n",
    "                                                        test_size=0.20,\n",
    "                                                        random_state = 10)\n",
    "            logit_reg = LogisticRegression()\n",
    "            logit_model = logit_reg.fit(X_train,\n",
    "                                        y_train.ravel())\n",
    "            \n",
    "            y_pred = logit_model.predict(X_test)\n",
    "\n",
    "            cf_matrix = confusion_matrix(y_test,y_pred,normalize = 'true')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two Combined Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# two models ([best text features + non-text features] * [democrat/republican, senator/representative])\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Why do standard preprocessing techniques need to be further customized to a particular corpus?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**YOUR ANSWER HERE** ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Did you find evidence for the idea that Democrats and Republicans have different sentiments in their tweets? What about Senators and Representatives?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**YOUR ANSWER HERE** ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Why is validating your exploratory and unsupervised learning approaches with a supervised learning algorithm valuable?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**YOUR ANSWER HERE** ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Did text only, non-text only, or text and non-text features together perform the best? What is the intuition behind combining text and non-text features in a supervised learning algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**YOUR ANSWER HERE** ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
